{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_to_complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a8b3f69610a4257baac737282c1f938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_574b7276064443318d5040598635b8a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9eead6cf41d14d00b29c822162f24db5",
              "IPY_MODEL_969b209c78e6424aa934e32ed6c9bcef"
            ]
          }
        },
        "574b7276064443318d5040598635b8a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9eead6cf41d14d00b29c822162f24db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b8730a4ad4f4ccab89ba6238abed93c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d5840d325f64a73b987d21457399197"
          }
        },
        "969b209c78e6424aa934e32ed6c9bcef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1012ff0271e54fe18c694a8ea611d12a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [16:38&lt;00:00, 9929.20it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30d282b137a64081b976b5de42ea18e5"
          }
        },
        "0b8730a4ad4f4ccab89ba6238abed93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d5840d325f64a73b987d21457399197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1012ff0271e54fe18c694a8ea611d12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30d282b137a64081b976b5de42ea18e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88a02534614d4f18a06c109dc35dd798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_471b149bfb1044e580b9630eb31bb830",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c9830294d9b74f9ba08ab4a8229437b3",
              "IPY_MODEL_33d1147fed514dde94116ea31be5f41b"
            ]
          }
        },
        "471b149bfb1044e580b9630eb31bb830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9830294d9b74f9ba08ab4a8229437b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_546282c661494442bfeba4f6e74d683f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21da06d0eec441a99cdefc953b7e6928"
          }
        },
        "33d1147fed514dde94116ea31be5f41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac27f6c3294748c2b562e5b27b3c28b1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:02&lt;00:00, 11352.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffb15ccbbc4d44699c35ca76462564c0"
          }
        },
        "546282c661494442bfeba4f6e74d683f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21da06d0eec441a99cdefc953b7e6928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac27f6c3294748c2b562e5b27b3c28b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffb15ccbbc4d44699c35ca76462564c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22cccebe387c4fd5a710fc8f766d554c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f2f7b7b857ec456c93683b5474582f17",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ff4b55dbab2412ea6c9d7cf5bb4d382",
              "IPY_MODEL_027582ea5d4f43d0b510e878e7fd04fd"
            ]
          }
        },
        "f2f7b7b857ec456c93683b5474582f17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ff4b55dbab2412ea6c9d7cf5bb4d382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a0e6d8213b74f31b492e2caaa5f6cca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2158e3a8fbb94e02afcd96dcd9bdf8c7"
          }
        },
        "027582ea5d4f43d0b510e878e7fd04fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fa753411eb3c4332a4e2a1b92020209c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:01&lt;00:00, 1079260.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5be17531f1c941a094954aa8629af601"
          }
        },
        "4a0e6d8213b74f31b492e2caaa5f6cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2158e3a8fbb94e02afcd96dcd9bdf8c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa753411eb3c4332a4e2a1b92020209c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5be17531f1c941a094954aa8629af601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fcb8103fcfaa443bbae362a9df822367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e166d44fd9b94bd09eef7ac5ef856022",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2d2de4e6bba4544ac0b112af8119612",
              "IPY_MODEL_815c323b6f4c445b854fad909bb99411"
            ]
          }
        },
        "e166d44fd9b94bd09eef7ac5ef856022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2d2de4e6bba4544ac0b112af8119612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_30feb9be0d99458ca29fe225cb1e56dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad491be7cfaa4e1491a53c61ff0f527c"
          }
        },
        "815c323b6f4c445b854fad909bb99411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_85aa1f504cb441cf8e8d76ce69b65277",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:33&lt;00:00, 153.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_263c1797b1524140b792a6014c3f3e7f"
          }
        },
        "30feb9be0d99458ca29fe225cb1e56dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad491be7cfaa4e1491a53c61ff0f527c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "85aa1f504cb441cf8e8d76ce69b65277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "263c1797b1524140b792a6014c3f3e7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpt31733/Deep-Leaning-MILA/blob/main/CNN_to_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_5PWr7JoIbf"
      },
      "source": [
        "# IVADO-Mila Deep Learning School\n",
        "# Spring 2021\n",
        "# Tutorial : Image classification and object detection with CNNs\n",
        "\n",
        "## Authors: \n",
        "\n",
        "\n",
        "Jeremy Pinto <jeremy.pinto@mila.quebec>\n",
        "\n",
        "Margaux Luck <margaux.luck@mila.quebec>\n",
        "\n",
        "Pierre Luc Carrier <pierre.luc.carrier@mila.quebec>\n",
        "\n",
        "Mathieu Germain <mathieu.germain@mila.quebec>\n",
        "\n",
        "### Translation to English: \n",
        "\n",
        "Laurent Charlin <lcharlin@gmail.com>\n",
        "\n",
        "## Intro\n",
        "\n",
        "The first part of this tutorial uses concrete examples to introduce the fundamental concepts behind convolutional neural networks.\n",
        "\n",
        "The second part of this tutorial is entirely optional but it covers more advanced tasks (object detection and instance segmentation) and it does so in a transfer learning setting. You are encouraged to attempt it if you wish to deepen your understanding of the material."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Cv_Nm9kekL"
      },
      "source": [
        "# Task 1 : Image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A73O9J3necuk"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PNXwEm-ej9a"
      },
      "source": [
        "Before we begin, you must ensure to install all required libraries for this part of the tutorial. To do so, we will use the `pip` utility. Simply execute the cell below by selecting it and pressing `shift`+`Enter`. (This operation may take a few minutes.)\n",
        "\n",
        "We need to be using the latest version of `pillow` for this tutorial. If you are prompted with:\n",
        "\n",
        "> WARNING: The following packages were previously imported in this runtime:\n",
        "  [PIL]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "\n",
        "Then click on restart runtime and rerun the cells afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNlYWG6z9GKT",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28f1932-c68e-4613-eeec-8d1c2e1a628e"
      },
      "source": [
        "!pip3 install torch torchvision matplotlib\n",
        "!pip3 install --upgrade pillow==8.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (8.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already up-to-date: pillow==8.1.0 in /usr/local/lib/python3.7/dist-packages (8.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmCDK-xlRmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070cbfe8-119b-402a-bdb4-8bb973c42e7c"
      },
      "source": [
        "import torch\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch version:  1.8.1+cu101\n",
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNomSpLO9LeH"
      },
      "source": [
        "## The MNIST dataset\n",
        "MNIST is the classic go-to **classification dataset** used in **computer vision**. It is available here: <a href=\"http://yann.lecun.com/exdb/mnist/\">Yann LeCun's website</a>. \n",
        "\n",
        "Each datum is an **image of a handwritten digit**. Here are a few examples from this dataset: \n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/mnist.png?raw=true)\n",
        "\n",
        "Each image also comes with a **class label** which indicates which digit does the image correspond to. For example, the labels of the examples above are 5, 0, 4, and 1 respectively. The classes are balanced which means that all digits appear (roughly) the same number of times in the dataset.\n",
        "\n",
        "The dataset is composed of **60 000 training examples** and **10 000 test examples**. All images have exactly the same size (**28x28 pixels** or 28 rows by 28 columns). Each pixel is represented by a number between 0 and 255 which represents its grey level (0 is white and 255 is black). Depending on the model, each image may have to be flattened (to a 784-length vector)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk9AMaT9hmcX"
      },
      "source": [
        "### Objective\n",
        "\n",
        "Our goal is to find a model that correctly classifies these images. In particular, our model will take as input (features/covariates/independant-variables) an image and will predict its digit (label/dependant variable). This problem can be formalized as follows:\n",
        "\n",
        "`f(image) = predicted digit`\n",
        "\n",
        "where `f` is a function.\n",
        "\n",
        "In this tutorial, we will consider both **multi-layer perceptrons (MLPs)** and **convolutional neural networks** as functions for solving this prediction problem. Both models take as inputs pixel intensities which will be modified using mathematical operations through the layers of the networks. Their output is a vector of size *1x10* where each entry corresponds to the (normalized) score that the input image is a particular digit. The sum over these 10 values is 1, and each score is non-negative. This is why these scores can be interpreted as probabilities. Our final prediction will be the entry with the highest score. For example, this prediction\n",
        "\n",
        "`[0.8, 0.1, 0, 0, 0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0]`\n",
        "\n",
        "indicates that the model assigns a score of 0.8 to class 0.\n",
        "\n",
        "Learning implies finding the parameters of a model that will maximize the model's performance. To learn, we will start by randomly initializing the parameters of our model. Then we iterate through examples. For each example we will obtain the network's prediction, compare it with the true label, and then update the parameters of the models to obtain a better prediction. We do this until we reach some predetermined stopping criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1U2sDz9ufy9"
      },
      "source": [
        "### Download the dataset and create the data loader\n",
        "\n",
        "Before we begin training, we have to obtain the MNIST dataset. It turns out that there are built-in functions within PyTorch to load this data, shuffle it and augment it.\n",
        "\n",
        "Here is an easy way to load some data in PyTorch: \n",
        "1. Subclass <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> and add  `__getitem__` and `__len__` methods.\n",
        "2. Then you can use<a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> to read and load the data into memory.\n",
        "\n",
        "It is even easier for MNIST in PyTorch since there is already a subclass of \"datasets\" defined for it: <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#mnist\">`torchvision.datasets.MNIST`</a>.\n",
        "\n",
        "<a href=\"http://pytorch.org/docs/master/torchvision/datasets.html\">Other datasets are also similarly available</a>\n",
        "\n",
        "**Note:** <a href=\"http://pytorch.org/docs/master/tensors.html#torch.Tensor.view\">`torch.Tensor.view()`</a> returns a new tensor with the same data as the original tensor but a different shape. For example, it can be used to flatten an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2iC4F8H8bsx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955,
          "referenced_widgets": [
            "6a8b3f69610a4257baac737282c1f938",
            "574b7276064443318d5040598635b8a3",
            "9eead6cf41d14d00b29c822162f24db5",
            "969b209c78e6424aa934e32ed6c9bcef",
            "0b8730a4ad4f4ccab89ba6238abed93c",
            "0d5840d325f64a73b987d21457399197",
            "1012ff0271e54fe18c694a8ea611d12a",
            "30d282b137a64081b976b5de42ea18e5",
            "88a02534614d4f18a06c109dc35dd798",
            "471b149bfb1044e580b9630eb31bb830",
            "c9830294d9b74f9ba08ab4a8229437b3",
            "33d1147fed514dde94116ea31be5f41b",
            "546282c661494442bfeba4f6e74d683f",
            "21da06d0eec441a99cdefc953b7e6928",
            "ac27f6c3294748c2b562e5b27b3c28b1",
            "ffb15ccbbc4d44699c35ca76462564c0",
            "22cccebe387c4fd5a710fc8f766d554c",
            "f2f7b7b857ec456c93683b5474582f17",
            "7ff4b55dbab2412ea6c9d7cf5bb4d382",
            "027582ea5d4f43d0b510e878e7fd04fd",
            "4a0e6d8213b74f31b492e2caaa5f6cca",
            "2158e3a8fbb94e02afcd96dcd9bdf8c7",
            "fa753411eb3c4332a4e2a1b92020209c",
            "5be17531f1c941a094954aa8629af601",
            "fcb8103fcfaa443bbae362a9df822367",
            "e166d44fd9b94bd09eef7ac5ef856022",
            "d2d2de4e6bba4544ac0b112af8119612",
            "815c323b6f4c445b854fad909bb99411",
            "30feb9be0d99458ca29fe225cb1e56dd",
            "ad491be7cfaa4e1491a53c61ff0f527c",
            "85aa1f504cb441cf8e8d76ce69b65277",
            "263c1797b1524140b792a6014c3f3e7f"
          ]
        },
        "outputId": "7c65b0e3-9660-40c6-deb2-a230564beeba"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import sampler, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "manualSeed = 1234\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# Fixing random seed\n",
        "random.seed(manualSeed)\n",
        "np.random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "if use_gpu:\n",
        "   torch.cuda.manual_seed_all(manualSeed)\n",
        "\n",
        "class ChunkSampler(sampler.Sampler):\n",
        "    \"\"\"Samples elements sequentially from some offset.\n",
        "    From: https://github.com/pytorch/vision/issues/168\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_samples: int\n",
        "      # of desired datapoints\n",
        "    start: int\n",
        "      Offset where we should start selecting from\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, start=0):\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start, self.start + self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "train_dataset = MNIST(root='../data', \n",
        "                      train=True, \n",
        "                      transform=transforms.ToTensor(),  \n",
        "                      download=True)\n",
        "\n",
        "test_dataset = MNIST(root='../data', \n",
        "                     train=False, \n",
        "                     transform=transforms.ToTensor())\n",
        "\n",
        "train_dataset_sizes = len(train_dataset)\n",
        "num_train_samples = int(0.8 * train_dataset_sizes)\n",
        "num_valid_samples = train_dataset_sizes - num_train_samples\n",
        "num_test_samples = len(test_dataset)\n",
        "\n",
        "print('# of train examples: {}'.format(num_train_samples))\n",
        "print('# of valid examples: {}'.format(num_valid_samples))\n",
        "print('# of test examples: {}'.format(num_test_samples))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          sampler=ChunkSampler(num_train_samples, 0),\n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False)\n",
        "\n",
        "valid_loader = DataLoader(dataset=train_dataset,\n",
        "                          sampler=ChunkSampler(\n",
        "                              num_valid_samples, num_train_samples),\n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                         batch_size=batch_size, \n",
        "                         shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a8b3f69610a4257baac737282c1f938",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a02534614d4f18a06c109dc35dd798",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22cccebe387c4fd5a710fc8f766d554c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcb8103fcfaa443bbae362a9df822367",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Processing...\n",
            "Done!\n",
            "# of train examples: 48000\n",
            "# of valid examples: 12000\n",
            "# of test examples: 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRV8zZbHV6zN"
      },
      "source": [
        "Let's visualize the training data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB57DZYzV9Oz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e0777df7-a549-44a2-a29c-9b3e6651ffd6"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inputs, labels = next(iter(train_loader))\n",
        "\n",
        "print('Inputs size: {}'.format(inputs.size()))\n",
        "print('Classes size: {}'.format(labels.size()))\n",
        "\n",
        "# Pick a random image of the batch\n",
        "idx = np.random.randint(len(inputs))\n",
        "img = 255 - inputs[idx] * 255\n",
        "\n",
        "# Plot the image\n",
        "print('\\n\\nDisplay a random image:')\n",
        "img_np = img.numpy()[0, :, :] # Discard the channel dimension\n",
        "plt.imshow(img_np, cmap='gray', vmin=0, vmax=255)\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# display the associated label\n",
        "print(f\"Label associated: {labels[idx]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs size: torch.Size([128, 1, 28, 28])\n",
            "Classes size: torch.Size([128])\n",
            "\n",
            "\n",
            "Display a random image:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label associated: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_XSUTbG0UvX"
      },
      "source": [
        "## CPU or GPU\n",
        "**Note:** <a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a library which can perform tensor operations using GPUs. Specifically, the library includes CUDA tensors which offer the same operations as regular tensors but instead run on GPUs, instead of CPUs.\n",
        "<a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns whether or not CUDA is available. Adding `.to(\"cuda:0\")` to the variable identifying a CPU tensor, returns the equivalent GPU tensor.\n",
        "\n",
        "For more information about using GPUs on colab, please refer to this [tutorial](https://colab.research.google.com/drive/1P7okDVh6viCIOkii6UAF2O9sTAcKGNWq).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxnZv9g_0RQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbe19df-fbc0-42ba-f7f0-ddccf987b835"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjWUALQGXQbi"
      },
      "source": [
        "We are going to compare the performance of a CNN to the performance of a vanilla MLP. Below is a quick reminder of MLPs, the architecture of the MLP we will be using, as well as the code to train the MLP. \n",
        "\n",
        "## Multi-layer perceptron (MLP)\n",
        "A multi-layer perceptron is a vanilla feed-forward neural network. Our instanciation will take as input an image, will transform it through a series of hidden layers and then will pass it to an output layer. This output is a vector of 10 numbers where each represents the normalized score of a particular class (this is sometimes interpreted as a probability).\n",
        "\n",
        "For example, here an MLP architecture to classify MNIST images: \n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/mlp.png?raw=true)\n",
        "\n",
        "Whenever you are trying to solve a prediction task, the process usually goes as follows: \n",
        "<ol>\n",
        "<li>Determine the network's artchitecture. This will implicitely determine the number of parameters (weights and biases) of the network.</li>\n",
        "<li>Determine the cost function and the optimization method.</li>\n",
        "<li>Train the weights of the network (i.e., fit the model to train data).</li>\n",
        "<li>Test the network (i.e., evaluate its performance on test data).</li>\n",
        "</ol>\n",
        "\n",
        "This procedure is general and applies to all types of (deep) neural networks.\n",
        "\n",
        "### Toolbox\n",
        "\n",
        "Racall that a (deep) neural network can be coded by using the library <a href=\"http://pytorch.org/docs/master/nn.html\">`torch.nn`</a>. `nn` uses <a href=\"http://pytorch.org/docs/master/autograd.html\">`torch.autograd`</a> to instantiate and compute the gradients (of the loss function with respect to the parameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho71dy2JXQbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e81d228-7959-4b7b-98db-ad8ecff49f94"
      },
      "source": [
        "import torch.nn as nn\n",
        "import copy\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "learning_rate = 1e-2\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.hidden_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "        \n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, num_classes)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):        \n",
        "        out = self.hidden_layer(x)\n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "# Define our model\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "model = model.to(device) # switch model to GPU\n",
        "\n",
        "print(model)\n",
        "print(\"\\n\\n Number of parameters in the model (weights and biases): \", sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "# Define the loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (hidden_layer): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (output_layer): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            " Number of parameters in the model (weights and biases):  648010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdh8ocHBXQbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7852df-bf81-49f9-97dc-f83955934fed"
      },
      "source": [
        "import time\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "        \n",
        "        # put images on proper device (GPU)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        # put images on proper device (GPU)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "        \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Statistics\n",
        "        valid_loss += loss.item()\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Start training #\n",
            "\n",
            "Epoch: 1/10\n",
            "\tTrain Loss: 2.1725\n",
            "\tValid Loss: 1.9143\n",
            "\n",
            "Epoch: 2/10\n",
            "\tTrain Loss: 1.3589\n",
            "\tValid Loss: 0.8584\n",
            "\n",
            "Epoch: 3/10\n",
            "\tTrain Loss: 0.6958\n",
            "\tValid Loss: 0.5438\n",
            "\n",
            "Epoch: 4/10\n",
            "\tTrain Loss: 0.5135\n",
            "\tValid Loss: 0.4407\n",
            "\n",
            "Epoch: 5/10\n",
            "\tTrain Loss: 0.4387\n",
            "\tValid Loss: 0.3903\n",
            "\n",
            "Epoch: 6/10\n",
            "\tTrain Loss: 0.3973\n",
            "\tValid Loss: 0.3600\n",
            "\n",
            "Epoch: 7/10\n",
            "\tTrain Loss: 0.3705\n",
            "\tValid Loss: 0.3392\n",
            "\n",
            "Epoch: 8/10\n",
            "\tTrain Loss: 0.3510\n",
            "\tValid Loss: 0.3234\n",
            "\n",
            "Epoch: 9/10\n",
            "\tTrain Loss: 0.3355\n",
            "\tValid Loss: 0.3104\n",
            "\n",
            "Epoch: 10/10\n",
            "\tTrain Loss: 0.3224\n",
            "\tValid Loss: 0.2994\n",
            "\n",
            "\n",
            "Training complete in 1m 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhO4qn_vXQbq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "eb16fba9-d2b6-4d79-a67a-6dc17720e735"
      },
      "source": [
        "# Save history for later\n",
        "mlp_train_loss_history = train_loss_history\n",
        "mlp_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, mlp_train_loss_history, label='train')\n",
        "plt.plot(x, mlp_valid_loss_history, label='valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W5NEuERXQbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0dd82a7-86b9-41f8-cc84-ae21d5dd6f42"
      },
      "source": [
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test data\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    # put images on proper device (GPU)\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    # Flatten the images\n",
        "    images = images.view(-1, 28*28)\n",
        "\n",
        "    # Forward\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    correct += torch.sum(predicted == labels.data)\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the test set: 91.32%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp1YgKZb8btG"
      },
      "source": [
        "## Convolutional neural networks (CNNs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMbU21dg3Rqx"
      },
      "source": [
        "### Concepts\n",
        "\n",
        "We first review the basic concepts that underlie CNNs.\n",
        "\n",
        "**Convolution**\n",
        "\n",
        "A convolution \"slides\" a filter *K* along image *I* to obtain an output *I*\\**K*.\n",
        "\n",
        "Here is an example of a 2D convolution:\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/numerical_no_padding_no_strides.gif?raw=true)\n",
        "\n",
        "\n",
        "**Filters**\n",
        "\n",
        "Filters (or kernels) are used to extract information useful to the task from their input. Filters are generally of size *n* \\* *n* where *n* is usually odd. The filters are parametrized by weights, one for each of its entry, which are learned by the convolutional network.\n",
        "\n",
        "The filter used in the previous example is:\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/d7acc4aeb74d9e9cb5fb51482a302196594837fe.png?raw=true)\n",
        "\n",
        "**Depth**\n",
        "\n",
        "We typically use *M* of filters which can be understood as the depth of the layer (see below). Note that this is different from the depth of the network (which is the number of layers). M is a hyperparameter. Here, each filter's output (blue circles) is represented as a single depth dimension on the output.\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/Conv_layer.png?raw=true)\n",
        "\n",
        "**Stride**\n",
        "\n",
        "The stride corresponds to the number of pixels the filter moves over in between each step of the convolution. The stride is measured in terms of a number of pixels. We typically use strides of 1 or 2. The larger the stride, the smaller the dimension of the output.\n",
        "\n",
        "**Zero padding**\n",
        "\n",
        "Zero padding consists in padding (adding) a border of zeros around the input image. This can be useful to preserve the dimension from input to output.\n",
        "\n",
        "Below is an example of a zero padding which preserves the dimensions from input to output. Here, zero padding is set to 1, stride is set to 1, and the filter has size 3x3.\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/same_padding_no_strides.gif?raw=true)\n",
        "\n",
        "\n",
        "**Max Pooling**\n",
        "\n",
        "In addition to convolutions, CNNs usually have pooling layers. The goal of pooling is to reduce the dimensionality of the input in-between two convolution layers to reduce the number of parameters in the network. For example, the famous LeNet CNN, uses max pooling with 2x2 filters and a stride of 2. Max pooling outputs the max value in a 2x2 region. This output is then the input of the next layer.\n",
        "\n",
        "Here is an example of the max pooling operation:\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/maxpool.jpeg?raw=true)\n",
        "\n",
        "\n",
        "**Receptive Field**\n",
        "\n",
        "The receptive field is a measure of the CNNs' capacity to perceive information at different input scales. In an MLP (fully connected), the features are connected to every neuron. The receptive field of this MLP is the full image.\n",
        "\n",
        "For CNNs, convolution operations typically imply sparse connections. In other words, each filter only has a local receptive field. However, each successive layer will have access to a slightly larger receptive field. \n",
        "\n",
        "Let's look at an example. Imagine a 3x3 filter with stride set to 1. In this case, the first layer's receptive field is a maximum of 3x3. However, the more layer we add the more we increase the network's receptive field. Adding a second layer with 3x3 filters and a stride of 1, our receptive field is increased to 5x5. Adding a third 3x3 layer further increases our receptive field to 7x7.\n",
        "\n",
        "What is the advantage of using multiple smaller successive filters instead of a single larger one? A single large filter of 7x7 implies 49 parameters. Instead 3 layers of 3x3 filters requires only 27 parameters (9 \\* 3). It is therefore more efficient to use multiple successive filters and in both cases the receptive field is the same (7x7). In addition, by using multiple successive filters, we can introduce more non-linearities in the model (one after each filter).\n",
        "\n",
        "Here the 3x3 filter (in grey) with a stride of 1 has a receptive field of 5x5 (yellow region):\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/sNBmKMKAz-yJeCuS14usSqw.png?raw=true)\n",
        "\n",
        "\n",
        "**Number of dimensions**\n",
        "\n",
        "In general, for a convolutional layers with input dimensions (volume)\n",
        "$W_1 * H_1 * D_1$ and hyperparameters \n",
        "\n",
        "* Number of filters = $K$\n",
        "\n",
        "* Filter sizes = $F$\n",
        "\n",
        "* Stride = $S$\n",
        "\n",
        "* Zero Padding = $P$.\n",
        "\n",
        "We obtain an output volume of $W_2 * H_2 * D_2$ dimensions where  \n",
        "\n",
        "* $W_2 = (W_1 - F + 2P) / S + 1$\n",
        "* $H_2 = (H1 - F + 2P) / S + 1$\n",
        "* $D_2 = K$\n",
        "\n",
        "and the total number of parameters is $(F⋅F⋅D_1)⋅K$ weights and $K$ biases.\n",
        "\n",
        "For an in-depth analysis see [this paper](http://cs231n.github.io/convolutional-networks/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owIcd9VguyAe"
      },
      "source": [
        "### LeNet architecture\n",
        "CNNs have been developed to model images. They can model images more efficiently (with fewer parameters) than an equivalent MLP. LeNet is a basic CNN for classification. It comes in several versions.\n",
        "\n",
        "We will use a \"LeNet 5\" to classify MNIST digit images:\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/lenet5.png?raw=true)\n",
        "\n",
        "\n",
        "To solve a prediction task, the process usually goes as follows (it is the same for MLPs and CNNs):\n",
        "<ol>\n",
        "<li>Determine the network's architecture. This will implicitly determine the number of parameters (weights and biases) of the network.</li>\n",
        "<li>Determine the cost function and the optimization method.</li>\n",
        "<li>Train the weights of the network (i.e., fit the model to train data).</li>\n",
        "<li>Test the network (i.e., evaluate its performance on test data).</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_24D5UJtJ-e"
      },
      "source": [
        "### Implementing the LeNet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXE2OZm8pqcI"
      },
      "source": [
        "#### Exercise 1\n",
        "\n",
        "In this section, you must fill in the missing sections in the LeNet implementation below.\n",
        "\n",
        "Recall that, to instantiate a particular network in PyTorch, one first subclasses <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module\">`torch.nn.Module`</a> and then writes the following methods :\n",
        "* The `__init__` method instantiates the various layers used in the model. \n",
        "* The `forward(input)` method takes in inputs to the model, passes them through the different layers of the model and returns the resulting outputs.\n",
        "\n",
        "For LeNet 5's `__init__` method, the following classes can be used:\n",
        "* <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Conv2d\">`torch.nn.Conv2d(in_channels, out_channels, kernel_size)`</a> applies a 2D convolution on the input channels.\n",
        "* <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.MaxPool2d\">`torch.nn.MaxPool2d(kernel_size)`</a> applies 2D max pooling on the input channels.\n",
        "* <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> applies a linear transformation on its input: y = Ax + b.\n",
        "* <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.ReLU\">`torch.nn.Relu()`</a> applies an elementwise Relu activation: Relu(x) = max(0, x).\n",
        "* <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\">`torch.nn.Sequential`</a> a sequential container in which to add modules in the order in which they will be constructed.\n",
        "\n",
        "Finally, `model.to(device)` passes the model to an available GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR4OQa-4gBAQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2871a6-287c-455f-c6cd-6f0f25bdff95"
      },
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.hidden_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,stride=1, padding=2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5,stride=1, padding=2),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(start_dim=0)\n",
        "            )\n",
        "        \n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(7*7*32, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.hidden_layers(x)\n",
        "        out = sel.output_layer(out)\n",
        "        return out\n",
        "        \n",
        "model = LeNet5()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet5(\n",
            "  (hidden_layers): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Flatten(start_dim=0, end_dim=-1)\n",
            "  )\n",
            "  (output_layer): Sequential(\n",
            "    (0): Linear(in_features=1568, out_features=10, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "# Parameters:  28938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRo03AR2PFPE"
      },
      "source": [
        "We note that LeNet5 has 28 938 parameters versus 648 010 parameters for an somewhat equivalent MLP with two hidden layers. This reduction in the number of parameters is significant. \n",
        "\n",
        "Here is how we calculate the number of parameters for LeNet5: \n",
        "\n",
        "```\n",
        "1st layer: 16 filters of size 5x5 + 16 biases = 16*5*5 + 16 = 416\n",
        "2nd layer: 16 * 32 filters of size 5x5 + 32 biases = 16*32*5*5 + 32 = 12 832\n",
        "FC layer: 7*7*32*10 + 10 biases = 15 690\n",
        " \n",
        "Total = 416 + 12 832 + 15 690 = 28 938\n",
        "```\n",
        "\n",
        "As a comparison, here is how we calculate the number of parameters of the two hidden layer MLP: \n",
        "The input flattens the 28x28 images into a vector of size 784. The second layer has 500 neurons. Each neuron requires 784 weights + 1 bias. So 500\\*785 parameters. This is then fed to another layer of 500 neurons which adds 501\\*500 parameters. Finally, the output layer has 10 neurons, each with 500 weights and a single bias for a total of 10\\*501 parameters. \n",
        "\n",
        "So in total we have: \n",
        "```\n",
        "500*785 + 501*500 + 10*501 = 648010\n",
        "``` parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7PEXQGtddk"
      },
      "source": [
        "### Selecting the cost function and the optimization method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9H0ssbh3V1j"
      },
      "source": [
        "#### Exercise 2\n",
        "\n",
        "In this section, you must implement the initialization of both the loss function to be optimized during training, and the optimizer to be used to train the model.\n",
        "\n",
        "The following are common choices for a multi-class classification task :\n",
        "* **Cost function :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. The cross entropy is often used in this context. It compares a (multivariate) distribution $p$ with a reference distribution $t$. It is minimized for $p=t$ and it is expressed mathematically by: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ are examples and $j$ the target class.\n",
        "* **Optimization method :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> a standard stochastic gradient descent (SGD) implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE3Cfrd-hJ0e"
      },
      "source": [
        "criterion = ... # To complete.\n",
        "\n",
        "optimizer = ... # To complete.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iG7go24tl93"
      },
      "source": [
        "\n",
        "### Training the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYEtUb4s3rab"
      },
      "source": [
        "#### Exercise 3\n",
        "\n",
        "In this section, you will need to complete the implementation of the model training loop.\n",
        "\n",
        "Training a network usually involves iterating for multiple epochs over the training dataset. One epoch corresponds to one pass over the full dataset. \n",
        "\n",
        "The dataset is usually divided into batches. Each epoch will then receive sequentially batches. For each batch we do the following operations:\n",
        "1. `optimizer.zero_grad()`: we clear the previously stored gradients.\n",
        "2. `loss.backward()`: we evaluate the cost, the gradients, and backpropagate the gradients through the computation graph.\n",
        "3. `optimizer.step()`: we update the parameters using the previously calculated gradients. For SGD, the update is: `weight = weight - learning_rate * gradient`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elxupovwhRSk"
      },
      "source": [
        "since = time.time()\n",
        "\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "\n",
        "        # put images on proper device (GPU)\n",
        "        images = ... # To complete.\n",
        "        labels = ... # To complete.\n",
        "        \n",
        "        # Zero the gradient buffer\n",
        "        ... # To complete.\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = ... # To complete.\n",
        "        \n",
        "        # Calculate loss using previously define criterion\n",
        "        loss = ... # To complete.\n",
        "        \n",
        "        # Backward pass\n",
        "        ... # To complete.\n",
        "        \n",
        "        # Optimize\n",
        "        ... # To complete.\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        # put images on proper device (GPU)\n",
        "        images = ... # To complete.\n",
        "        labels = ... # To complete.\n",
        "        \n",
        "        # Forward\n",
        "        outputs = ... # To complete.\n",
        "        \n",
        "        # Calculate loss using previously defined criterion\n",
        "        loss = ... # To complete.\n",
        "       \n",
        "        # Statistics\n",
        "        valid_loss += loss.item()\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fozzXmdRGTs"
      },
      "source": [
        "Let's plot the training curves!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUGbFeg5RHFZ"
      },
      "source": [
        "# Save history for later\n",
        "lenet5_train_loss_history = train_loss_history\n",
        "lenet5_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, lenet5_train_loss_history, label='train')\n",
        "plt.plot(x, lenet5_valid_loss_history, label='valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6kWtULrSDXL"
      },
      "source": [
        "We can overlay the validation curves on top of the training curves for training of LeNet5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE1qsmvaSTjH"
      },
      "source": [
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, mlp_train_loss_history, label='MLP train', linestyle='--')\n",
        "plt.plot(x, mlp_valid_loss_history, label='MLP valid', linestyle='--')\n",
        "plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n",
        "plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2T7JiZCtvJ4"
      },
      "source": [
        "### Testing the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq79RTld3xyc"
      },
      "source": [
        "#### Exercise 4\n",
        "\n",
        "In this section, we evaluate the network's performance on test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da47-MilhpN7"
      },
      "source": [
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over data.\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    # put images on proper device (GPU)\n",
        "    images = ... # To complete.\n",
        "    labels = ... # To complete.\n",
        "    \n",
        "    # No need to flatten the images here !\n",
        "\n",
        "    # Forward\n",
        "    outputs = ... # To complete.\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    batch_correct = ... # To complete.\n",
        "    correct += batch_correct\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voN-_iO_RQ8A"
      },
      "source": [
        "The best results are obtained after 10 epochs!\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eoh0pPzKuFKv"
      },
      "source": [
        "\n",
        "### Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFj_W39u5voa"
      },
      "source": [
        "#### Exercise 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chov57bzu76J"
      },
      "source": [
        "Batch normalization is a trick that often yields faster training. It acts as a regularizer by normalizing the inputs (by batch). Further, the operation is differentiable. For additional information see [article](https://arxiv.org/pdf/1502.03167v3.pdf).\n",
        "\n",
        "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/CNN/images/1_Hiq-rLFGDpESpr8QNsJ1jg.png?raw=true)\n",
        "\n",
        "\n",
        "Batch normalization is already implemented in the Pytorch framework so there is no need to implement it ourselves. To use it in our LeNet model, we simply treat it like any other layer : instantiate it in `__init__` and then use it in `forward`. The following class can be used:\n",
        "* <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm2d\">`nn.BatchNorm2d(num_features)`</a>: add batch normalisation to a 4-dimensional input encoded in a 3-dimensional tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pi_mhvg8E4E"
      },
      "source": [
        "class LeNet5_BatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5_BatchNorm, self).__init__()\n",
        "\n",
        "        ... # To complete.\n",
        "                \n",
        "    def forward(self, x):\n",
        "\n",
        "        ... # To complete.\n",
        "               \n",
        "        return out\n",
        "        \n",
        "model = LeNet5_BatchNorm()\n",
        "model = model.to(device)\n",
        "  \n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "271HrQBNccH1"
      },
      "source": [
        "Note that batch normalization adds parameters. Our new LeNet5 with batch normalization has 29 034 parameters (versus 28 938 for the original LeNet5 model without batch normalization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj4R3qV5ABFC"
      },
      "source": [
        "**The rest (i.e., the cost function, the optimizer, the training loops, and the testing procedures) remain unchanged!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgEcoSRQAVtA"
      },
      "source": [
        "criterion = ... # To complete.\n",
        "optimizer = ... # To complete.\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "\n",
        "        # put images on proper device (GPU)\n",
        "        images = ... # To complete.\n",
        "        labels = ... # To complete.\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        ... # To complete.\n",
        "        \n",
        "        # Forward\n",
        "        outputs = ... # To complete.\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = ... # To complete.\n",
        "        \n",
        "        # Backward pass\n",
        "        ... # To complete.\n",
        "        \n",
        "        # Optimize\n",
        "        ... # To complete.\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        images = ... # To complete.\n",
        "        labels = ... # To complete.\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = ... # To complete.\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = ... # To complete.\n",
        "      \n",
        "        # Statistics\n",
        "        valid_loss += loss.item()\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pxvS_yUeWog"
      },
      "source": [
        "We obtain even better results after 10 epochs!\n",
        "\n",
        "Let's have a look at the training and validation curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrv-o7Cle0ty"
      },
      "source": [
        "# Save history for later\n",
        "lenet5_batchnorm_train_loss_history = train_loss_history\n",
        "lenet5_batchnorm_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, mlp_train_loss_history, label='MLP train', linestyle='--')\n",
        "plt.plot(x, mlp_valid_loss_history, label='MLP valid', linestyle='--')\n",
        "plt.plot(x, lenet5_train_loss_history, label='LeNet5 train', linestyle='-.')\n",
        "plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid', linestyle='-.')\n",
        "plt.plot(x, lenet5_batchnorm_train_loss_history, label='LeNet5 BatchNorm train')\n",
        "plt.plot(x, lenet5_batchnorm_valid_loss_history, label='LeNet5 BatchNorm valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Du89Bk9ekuRF"
      },
      "source": [
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over data.\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    # put images on proper device (GPU)\n",
        "    images = ... # To complete.\n",
        "    labels = ... # To complete.\n",
        "    \n",
        "    # No need to flatten the images here !\n",
        "\n",
        "    # Forward\n",
        "    outputs = ... # To complete.\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    batch_correct = ... # To complete.\n",
        "    correct += batch_correct\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoyKsR7WGxyG"
      },
      "source": [
        "# Task 2 : Object detection and instance segmentation (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u61uTIT2fQfE"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "We need to be using the latest version of `pillow` for this section of the tutorial. If you are prompted with:\n",
        "\n",
        "> WARNING: The following packages were previously imported in this runtime:\n",
        "  [PIL]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "\n",
        "Then click on restart runtime and rerun the cells afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxF52IVYaGD8"
      },
      "source": [
        "!pip install --upgrade pillow==8.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkl0MmCwPhZJ"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    print(\"Using the following accelerator type: {}\".format(torch.cuda.get_device_name(0)))\n",
        "else:\n",
        "    print(\"No GPU/TPU found, try changing the runtime type if using colab.\")\n",
        "\n",
        "manualSeed = 1234\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# Fixing random seed\n",
        "random.seed(manualSeed)\n",
        "np.random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "if use_gpu:\n",
        "    torch.cuda.manual_seed_all(manualSeed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ixksNJiaIiU"
      },
      "source": [
        "## Object Detection Introduction\n",
        "Object detection involves locating where an object is in a given image using a bounding box. The box as well as the object type are predicted by the neural network. In the case of object detection, the box is typically rectangle. While a rectangle is composed of 4 (x,y) coordinates, only 4 values are needed to reconstruct a box. For example, we could use the top right (x,y) coordinates as well as the width and height of the box. We can also use the minimum and maximum values that x and y can take: [`x_min`, `y_min`, `x_max`, `y_max`]. There can be many objects in an image, and object detectors should be able to detect all instances of various objects.\n",
        "\n",
        "Here is an example of an image with it's associated bounding boxes.\n",
        "[bbox image exmaple]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRH7Y5lyxIaY"
      },
      "source": [
        "### Pretrained object detector\n",
        "\n",
        "Before we implement our own custom object detector, let's look at how an already pre-trained object detector works. We will use a pretrained object detector from [torchvision](https://pytorch.org/vision/stable/index.html) which was trained on the [COCO](https://cocodataset.org/#home) dataset.\n",
        "\n",
        "We will try detecting the objects in the following image:\n",
        "\n",
        "![Image1](https://github.com/jerpint/ivado-mila-dl-school-2019-vancouver/blob/dlschool21/assests/image1.jpg?raw=true)\n",
        "\n",
        "To do so, run the following cells:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x53FMnyYdyK"
      },
      "source": [
        "'''Test our pretrained object detector on an example image'''\n",
        "import os\n",
        "from urllib.request import urlopen\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "\n",
        "# Order of COCO category names as defined in torchvision\n",
        "COCO_CATEGORY_NAMES = [\n",
        "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
        "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
        "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
        "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
        "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
        "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
        "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "def display_image_and_boxes(image, target=None, category_names=None):\n",
        "    ''' \n",
        "    image: tensor\n",
        "    target: dict\n",
        "    category_names: list mapping class label (int) to class name (str)\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # Convert tensor to image and draw it.\n",
        "    np_img = (image.permute(1,2,0).cpu().numpy() * 255).astype('uint8')\n",
        "    im = Image.fromarray(np_img)\n",
        "    draw = ImageDraw.Draw(im)\n",
        "\n",
        "    if target:\n",
        "        # Make sure the required font is available\n",
        "        if not os.path.isfile('Roboto-Regular.ttf'):\n",
        "            !wget https://github.com/jerpint/ivado-mila-dl-school-2019-vancouver/raw/dlschool21/assests/Roboto-Regular.ttf\n",
        "        font = ImageFont.truetype(font='Roboto-Regular.ttf', size=16)\n",
        "\n",
        "        # Draw each bounding box in the target\n",
        "        for box, label in zip(target['boxes'], target['labels']):\n",
        "            box = box.detach().cpu().numpy()\n",
        "            draw.rectangle(box, outline='black')\n",
        "            label_str =  category_names[label.cpu().numpy()] if category_names else str(label.cpu().numpy()+2)\n",
        "            draw.text((box[0], box[1]), label_str, fill=(0,128,256,256), font=font)\n",
        "    return im\n",
        "\n",
        "\n",
        "def display_masks(image, predictions, category_names=None):\n",
        "    ''' \n",
        "    image: tensor\n",
        "    predictions: dict\n",
        "    category_names: list mapping class label (int) to class name (str)\n",
        "    \n",
        "    '''\n",
        "    font = ImageFont.truetype(font='Roboto-Regular.ttf', size=16)\n",
        "    mask_arr = np.zeros((256, 300))\n",
        "    im = Image.fromarray(mask_arr)\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    for box, label, mask in zip(predictions['boxes'], predictions['labels'], predictions['masks']):\n",
        "        mask = mask.detach().cpu().numpy().squeeze()\n",
        "        mask_arr += mask\n",
        "        box = box.detach().cpu().numpy()\n",
        "        draw.rectangle(box, outline='black')\n",
        "        label_str =  category_names[label.cpu().numpy()] if category_names else str(label.cpu().numpy()+2)\n",
        "        draw.text(((box[0] + box[2])/2, (box[1]+box[3])/2), label_str, fill=1, font=font)\n",
        "    mask_arr = (mask_arr > 0.5) * 255\n",
        "    im_with_mask = Image.fromarray( (np.array(im) + mask_arr).astype('uint8') )\n",
        "    return im_with_mask\n",
        "\n",
        "\n",
        "def URL_to_tensor(URL):\n",
        "    '''\n",
        "    Convert an image URL to a pytorch tensor.\n",
        "\n",
        "    Reorders the image axes so that channel is the first dimension and normalizes \n",
        "    channels to be in range [0,1].\n",
        "\n",
        "    input\n",
        "    -----\n",
        "    URL (str): URL of the image to be processed\n",
        "\n",
        "    returns\n",
        "    -------\n",
        "    img_tensor (list): list containing the image as a torch.tensor()\n",
        "    '''\n",
        "    \n",
        "    img = Image.open(urlopen(URL)) # Load image\n",
        "    img_tensor = torch.as_tensor(np.array(img) / 255) # Normalize input to [0, 1]\n",
        "    img_tensor = img_tensor.permute(2, 0, 1).float() # Reorder image axes to channel first\n",
        "    return [img_tensor]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJCT1hE6v4o"
      },
      "source": [
        "# Download sample image and convert to tensor\n",
        "URL = 'http://farm3.staticflickr.com/2462/3889476537_9e1a7c6af4_z.jpg'\n",
        "img_tensors = URL_to_tensor(URL)\n",
        "\n",
        "# Obtain model prediction\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
        "    pretrained=True,\n",
        "    box_score_thresh=0.7\n",
        ")\n",
        "model.eval()\n",
        "predictions = model(img_tensors) \n",
        "\n",
        "# Display image along with model prediction\n",
        "display_image_and_boxes(img_tensors[0], predictions[0], category_names=COCO_CATEGORY_NAMES)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHiQjWcyxwWx"
      },
      "source": [
        "The architecture used in this case is Faster R-CNN with a ResNet-50 backbone. It is considered a two-stage object detector since there are 2 components to the network: a region-proposal network and a network which predicts bounding-boxes and object categories:\n",
        "\n",
        "![fasterrcnn](https://github.com/jerpint/ivado-mila-dl-school-2019-vancouver/blob/dlschool21/assests/fasterrcnn.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOheu06BaFE1"
      },
      "source": [
        "## Object Detection of Polygons\n",
        "\n",
        "We will now explore using a new dataset to train a custom object detector. We will be constructing a synthetic polygons dataset. Our dataset will generate images with regular polygons of various colours scattered accross an image. Our object detector will have to find the polygons using bounding boxes as well as determine how many sides each polygon has.\n",
        "\n",
        "For example, let's consider an image scattered with regular polygons of different shapes & sizes:\n",
        "\n",
        "![polygons_nolabs](https://github.com/jerpint/ivado-mila-dl-school-2019-vancouver/raw/dlschool21/assests/polygons_nolabel.png)\n",
        "\n",
        "Our object detector will need to find and differentiate all different instances of polygons. Note that the polygons can be overlapping, rotated, of different colors, etc. The output of our detector will look like this:\n",
        "\n",
        "![polygon_labels](https://github.com/jerpint/ivado-mila-dl-school-2019-vancouver/raw/dlschool21/assests/polygons_labels.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AaiMyASuZxq"
      },
      "source": [
        "### Polygons Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8DnLPmECiJY"
      },
      "source": [
        "#### Exercise 6\n",
        "\n",
        "We will start by defining our own custom `Polygons` dataset, which we will define as a `torch.dataset`.\n",
        "\n",
        "You will be responsible to complete the following methods:\n",
        "\n",
        "`self.mask_to_bbox` and `self.img_to_tensor`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO-PhS4zuAW3"
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "class Polygons(Dataset):\n",
        "  \n",
        "    def __init__(self, \n",
        "                 im_width, \n",
        "                 im_height, \n",
        "                 dataset_len=100, \n",
        "                 max_polygon_size=6,\n",
        "                 max_polygons_per_image=8,\n",
        "                 split='train',\n",
        "                 background=False):\n",
        "        self.width = im_width\n",
        "        self.height = im_height\n",
        "        self.max_polygon_side=max_polygon_size\n",
        "        self.dataset_len = dataset_len\n",
        "        self.max_polygons_per_image = max_polygons_per_image\n",
        "        self.split = split\n",
        "        self.background = background\n",
        "        self.set_offset()\n",
        "\n",
        "    def set_offset(self):\n",
        "        '''The offset determines where the random seed index begins. \n",
        "\n",
        "        this ensures reproducibility accross runs.\n",
        "        '''\n",
        "        if self.split == 'train':\n",
        "            self.offset = 0\n",
        "        elif self.split == 'val':\n",
        "            self.offset = dataset_len\n",
        "        elif self.split == 'test':\n",
        "            self.offset = 2 * dataset_len\n",
        "        else:\n",
        "            raise ValueError(\"split has to be one of 'train', 'test', 'val'\")\n",
        "\n",
        "    def draw_polygon(self):\n",
        "        '''\n",
        "        Draws a single polygon.\n",
        "        \n",
        "        returns:\n",
        "        bbox: coordinates of the bounding box [xmin, ymin, xmax, ymax]\n",
        "        mask: Mask of the polygon, non-zero values where there is a polygon\n",
        "        label: the number of sides associated to the polygon\n",
        "        '''\n",
        "        img = Image.new('L', (self.width, self.height), 0)\n",
        "        n_sides = random.randint(3, self.max_polygon_side)\n",
        "        circle_x = random.randint(10, self.width-10)\n",
        "        circle_y = random.randint(10, self.height-10)\n",
        "        circle_r = random.randint(10, 60)\n",
        "        bounding_circle = (circle_x, circle_y, circle_r)\n",
        "        rotation = random.randint(0, 360)\n",
        "        ImageDraw.Draw(img).regular_polygon(bounding_circle=bounding_circle, n_sides=n_sides, rotation=rotation, fill=1, outline=None)\n",
        "        mask = np.array(img) \n",
        "        bbox = self.mask_to_bbox(mask)\n",
        "        label = n_sides - 2 # labels should be in range (1 ... N)\n",
        "        return bbox, mask, label\n",
        "\n",
        "    def add_random_background(self, img, seed=None):\n",
        "        '''\n",
        "        Add a random background to an existing image using gadients. \n",
        "        Adapted from: https://python-catalin.blogspot.com/2013/10/how-to-make-color-gradient-and-images.html\n",
        "\n",
        "        img: Original image with white background\n",
        "        img_with_bg: New image with a random background added to it\n",
        "        '''\n",
        "\n",
        "        if seed:\n",
        "            random.seed(seed)\n",
        "        bg = Image.new(\"RGB\", (self.width, self.height), \"#FFFFFF\")\n",
        "        draw = ImageDraw.Draw(bg)\n",
        "\n",
        "        # create the background\n",
        "        r,g,b = random.randint(0,255), random.randint(0,255), random.randint(0,255)\n",
        "        dr = (random.randint(0, 255) - r) / self.width\n",
        "        dg = (random.randint(0, 255) - g) / self.width\n",
        "        db = (random.randint(0, 255) - b) / self.width\n",
        "        for i in range(self.width):\n",
        "            r, g, b = r + dr, g + dg, b + db\n",
        "            draw.line((i, 0 , i, self.width), fill=(int(r), int(g), int(b)))\n",
        "          \n",
        "        # Add it to the original image\n",
        "        bg_array, img_array = np.array(bg), np.array(img)\n",
        "        bg_mask = np.zeros(bg_array.shape)\n",
        "        bg_mask[np.where(img_array == 255)] = 1\n",
        "        img_with_bg = Image.fromarray((img_array + bg_mask * bg_array).astype('uint8'))\n",
        "        \n",
        "        return img_with_bg\n",
        "    \n",
        "    def array_to_img(self, array):\n",
        "        '''Convert the numpy array data to an PIL image object.\n",
        "        i.e. 3 color channels [0, 255]\n",
        "        \n",
        "\n",
        "        inputs:\n",
        "        array: np.array\n",
        "\n",
        "        returns:\n",
        "        PIL.Image\n",
        "        '''\n",
        "        cm = plt.get_cmap('cubehelix') # Use a colormap for mapping int to colour\n",
        "        colored_image = cm(1 - array / 255)\n",
        "        return Image.fromarray((colored_image[:, :, :3] * 255).astype(np.uint8))\n",
        "\n",
        "    def img_to_tensor(self, img):\n",
        "        '''\n",
        "        convert a pil image to torch tensor.\n",
        "        Must scale values to be in range 0-1. Must return a float() tensor\n",
        "        reorder axes so that channel is first dimension.\n",
        "\n",
        "        img: PIL.Image\n",
        "\n",
        "        return: torch.tensor()\n",
        "        '''\n",
        "        \n",
        "        # Convert the PIL Image to a Torch Tensor. Make sure to properly\n",
        "        # implement all requirements stated in the docstring to avoid issues\n",
        "        # with model training later on.\n",
        "        ... # To complete.\n",
        "\n",
        "        return img_tensor\n",
        "\n",
        "    def mask_to_bbox(self, mask):\n",
        "        '''\n",
        "        Converts the polygon mask to bounding box coordinates.\n",
        "\n",
        "        mask: np.array\n",
        "\n",
        "        returns\n",
        "        bbox (tuple): (xmin, ymin, xmax, ymax)\n",
        "        '''\n",
        "\n",
        "        xmin = np.min(np.nonzero(mask)[1])\n",
        "        xmax = np.max(np.nonzero(mask)[1])\n",
        "        ymin = np.min(np.nonzero(mask)[0])\n",
        "        ymax = np.max(np.nonzero(mask)[0])\n",
        "        return (xmin, ymin, xmax, ymax)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        seed = idx + self.offset\n",
        "        random.seed(seed) # For reproducible datasets\n",
        "        n_polygons = random.randint(1, self.max_polygons_per_image) # At least one polygon per image\n",
        "        img_array = np.zeros((self.height, self.width))\n",
        "        masks = []\n",
        "        labels = []\n",
        "        boxes = []\n",
        "        areas = []\n",
        "        for _ in range(n_polygons):\n",
        "            bbox, mask, label = self.draw_polygon()\n",
        "            boxes.append(bbox)\n",
        "            masks.append(mask > 0) # Binary mask\n",
        "            areas.append((bbox[2]-bbox[0]) * (bbox[3]-bbox[1]))\n",
        "            labels.append(label)\n",
        "            img_array += mask / np.max(mask) * random.randint(0, 254) # random color\n",
        "\n",
        "        # Convert array to RGB image\n",
        "        img = self.array_to_img(img_array)\n",
        "\n",
        "        # Add background if specified\n",
        "        if self.background:\n",
        "            img = self.add_random_background(img, seed)\n",
        "\n",
        "        # Convert img to tensor\n",
        "        img_tensor = self.img_to_tensor(img) \n",
        "\n",
        "        # Define targets appropriately as tensors\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        target[\"masks\"] = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "        target[\"image_id\"] = torch.tensor([idx])\n",
        "        target[\"area\"] = torch.as_tensor(areas, dtype=torch.int64)\n",
        "        target[\"iscrowd\"] = torch.as_tensor([0] * len(labels), dtype=torch.int64)\n",
        "\n",
        "        return img_tensor, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_len \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy9hHAcJwldo"
      },
      "source": [
        "Lets start by exploring what the dataset does. For each index, it generates a new image with a random assortment of polygons and their associated labels.\n",
        "\n",
        "The labels are binary masks that represent each polygon as well as class labels that represent the number of sides of each polygon. Since pytorch indexes classes at 0, and we require a background class, the class associated to a polygon is  `class+2` when `class` > 1, and `background` otherwise.\n",
        "\n",
        "If you have properly implemented `self.img_to_tensor`, running the cell below will correctly display a sample from the Polygon dataset. You should see polygons of various colors over a white background."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYKB0gk_qc8N"
      },
      "source": [
        "polygon_dataset = Polygons(im_width=300, im_height=256)\n",
        "img, target = polygon_dataset[490]\n",
        "\n",
        "im = display_image_and_boxes(img)\n",
        "display(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSWFed2nz507"
      },
      "source": [
        "If you have properly implemented `self.mask_to_bbox`, running the cell below will correctly display the same sample example as before but with tight bounding boxes around each of the polygons in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYh_UROw1Clr"
      },
      "source": [
        "polygon_dataset = Polygons(im_width=300, im_height=256)\n",
        "img, target = polygon_dataset[490]\n",
        "\n",
        "im = display_image_and_boxes(img, target)\n",
        "display(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt2QA2Lwpa67"
      },
      "source": [
        "Finally, the cell below demonstrates another feature of the Polygons class. By specifying `background=True`, the Polygons class will generate example with non-white backgrounds in order to make the detection task more difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFJxXYWjTg8I"
      },
      "source": [
        "polygon_ds_with_bg = Polygons(im_width=300, im_height=256, background=True)\n",
        "img, target = polygon_ds_with_bg[4921]\n",
        "\n",
        "im = display_image_and_boxes(img, target)\n",
        "display(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPjy8tC3nJrY"
      },
      "source": [
        "### Fetching utility files\n",
        "\n",
        "We will be using some handy utility functions directly from torchvision's repo. Running the cell below will fetch them and store them in your working directory. It will also open a window so you can look at what some of the functions do. Most of it is to run training loops and to evaluate performances. No need to reinvent the wheel!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahTWdM4WcHyC"
      },
      "source": [
        "# Fetch some useful util files provided by torchvision\n",
        "# Open them in a side view\n",
        "from google.colab import files\n",
        "fnames = ['engine.py', 'utils.py', 'coco_utils.py', 'coco_eval.py', 'transforms.py']\n",
        "for f in fnames:\n",
        "    URL = \"https://raw.githubusercontent.com/pytorch/vision/master/references/detection/\" + f\n",
        "    !wget -O {f} {URL}\n",
        "    files.view(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9zd9s6vulpU"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKzifdf0YH6R"
      },
      "source": [
        "#### Exercise 7\n",
        "\n",
        "We will now need to define the model that we will be using to train our object detector. It will be a slight variant from Faster R-CNN with the main difference being the number of possible classes that the network can output.\n",
        "\n",
        "Since the overall architecture of Faster R-CNN can be rather complex, we will simply replace the layer responsible for bounding box regression and classification to be adapted to our task.\n",
        "\n",
        "We could directly call the `fasterrcnn_resnet50_fpn` model with the appropriate number of classes, however, this would not allow us to use a pretrained model.\n",
        "\n",
        "Instead, we can use the `FastRCNNPredictor` class and replace the `box_predictor` portion of the network. Looking at the [documentation](https://pytorch.org/vision/stable/_modules/torchvision/models/detection/faster_rcnn.html#fasterrcnn_resnet50_fpn), `FastRCNNPredictor` expects the number of input channels as well as the number of output channels. In our case, the number of output channels will be the number of classes we have to predict (including the background). The number of input channels will remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkkDLfwktFgw"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "def get_model_object_detection(num_classes, pretrained=True, box_score_thresh=0.5):\n",
        "    ## Load the pretrained fasterrcnn_resnet50_fpn model and specify box_score_thresh as a kwarg\n",
        "    model = ... # To complete.\n",
        "    \n",
        "    ## Determine number of input features for new FastRCNNPredictor\n",
        "    predictor_in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    \n",
        "    # Replace the model's classifier with a new instance of FastRCNNPredictor,\n",
        "    # that has the number of classes changed to what we are doing in this task\n",
        "    model.roi_heads.box_predictor = ... # To complete.\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHpciqXav9xv"
      },
      "source": [
        "Now that we have defined our model, let's see what kind of predictions our model is making prior to re-training our model. To do so, we will define a data loader, set the model to `eval` and do a forward pass on a single example image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARiM6OObSM_7"
      },
      "source": [
        "import utils\n",
        "# Do a prediction on the model prior to fine tuning\n",
        "polygon_ds = Polygons(im_width=300, im_height=256)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " polygon_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)\n",
        "\n",
        "num_classes = polygon_ds.max_polygon_side - 2\n",
        "model = get_model_object_detection(num_classes)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Show a batch worth of images and their predictions\n",
        "images, _ = next(iter(data_loader))\n",
        "predictions = model(images)           # Returns predictions\n",
        "for n in range(len(predictions)):\n",
        "    display(display_image_and_boxes(images[n], predictions[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laBaFbGovdLK"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTnMytxc-7I8"
      },
      "source": [
        "#### Exercise 8\n",
        "\n",
        "You will now implement the training loop for our object detector. We will use the `train_one_epoch` utility provided by torchvision.\n",
        "\n",
        "First, you will have to define a train and validation dataset, and construct appropriate dataloaders for them. Do not forget to use the dataset's `split` argument. Also do not forget to use the `colate_fn` provided in `utils.py` as the `collate_fn` argument in the `DataLoader` object.\n",
        "\n",
        "Use a `dataset_len` of 100 for each dataset. It can be useful to shuffle the train dataset. Use `im_width` = 300 and `im_height` = 256.\n",
        "\n",
        "For the optimizer, we will use `SGD` with an initial learning rate of 0.005. Set the momentum to be 0.9 and the weight_decay to be 0.0005 (as in the original faster RCNN implementation).\n",
        "\n",
        "We will also use a scheduler to decrease the learning rate on every step. For this, we will use the `torch.optim.lr_scheduler.StepLR` utility. Set the step_size to be 3 and the gamma to 0.1. This will decrease the learning rate for us automatically throughout our training.\n",
        "\n",
        "We will train for 5 epochs total."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTuDqPFEcPL0"
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "\n",
        "dataset_len = 100\n",
        "im_width = 300\n",
        "im_height = 256\n",
        "batch_size=2\n",
        "\n",
        "# Instantiate training dataset and training dataloader\n",
        "dataset_train = Polygons(im_width, im_height, dataset_len=dataset_len, split='train')\n",
        "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=utils.collate_fn)\n",
        "\n",
        "# Instantiate validation dataset and validation dataloader\n",
        "dataset_val = ... # To complete.\n",
        "data_loader_val = ... # To complete.\n",
        "\n",
        "\n",
        "# Instantiate test dataset and test dataloader\n",
        "dataset_test = ... # To complete.\n",
        "data_loader_test = ... # To complete.\n",
        "\n",
        "\n",
        "# get the model using our helper function (get_model_object_detection)\n",
        "num_classes = dataset_train.max_polygon_side - 1\n",
        "model = ... # To complete.\n",
        "\n",
        "# Set trainable parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# construct an optimizer\n",
        "optimizer = ... # To complete.\n",
        "\n",
        "# construct a learning rate scheduler\n",
        "lr_scheduler = ... # To complete.\n",
        "\n",
        "# set the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "print(f\"Using {device} as accelerator harward\")\n",
        "\n",
        "# train model for specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
        "\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # evaluate on the validation dataset\n",
        "    evaluate(model, data_loader_val, device=device)\n",
        "\n",
        "print(\"End of training.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4R2EwZl0e7d"
      },
      "source": [
        "### Model evaluation\n",
        "\n",
        "Now set your model to `eval` and define a test dataset and dataloader to evaluate the model on the test set.\n",
        "\n",
        "How is your model performing overall? Does it seem to generalize well from the training set? Does it perform better, worse, same, on different sized objects?\n",
        "\n",
        "It can be helpful to visually inspect how your network is performing by plotting some of the object detection results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E3tfA8pBlbf"
      },
      "source": [
        "model.eval()\n",
        "evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "# Look at predictions from a batch\n",
        "test_iter = iter(data_loader_test)\n",
        "images, targets = next(test_iter)\n",
        "images = [image.to(device) for image in images]\n",
        "predictions = model(images)\n",
        "for idx in range(len(images)):\n",
        "    display(display_image_and_boxes(images[idx], predictions[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnHorFIcH1Cl"
      },
      "source": [
        "### Generalization\n",
        "\n",
        "We will now see how our model generalizes to \"unusual\" cases. So far, our model has only seen shapes on white backgrounds.\n",
        "\n",
        "Lets add a random gradient-based background to the test dataset and see how well our model does. This can be done by setting the `background=True` argument in the dataset class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2cfxYz5BxvC"
      },
      "source": [
        "# Use a test set with a random background this time\n",
        "dataset_len = 100\n",
        "polygon_ds_with_bg = Polygons(im_width=300, \n",
        "                              im_height=256, \n",
        "                              dataset_len=dataset_len, \n",
        "                              background=True, \n",
        "                              split='test')\n",
        "\n",
        "\n",
        "data_loader_test_bg = torch.utils.data.DataLoader(\n",
        "    polygon_ds_with_bg, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "# Evaluate on entire test set\n",
        "evaluate(model, data_loader_test_bg, device=device)\n",
        "\n",
        "# Evaluate single sample\n",
        "bg_iter = iter(data_loader_test_bg)\n",
        "images, targets = next(bg_iter)\n",
        "\n",
        "model.eval()\n",
        "images = [image.to(device) for image in images]\n",
        "predictions = model(images)           # Returns predictions\n",
        "\n",
        "display_image_and_boxes(images[0], predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVju5xude5ke"
      },
      "source": [
        "Analysis of the results : \n",
        "- How does the model react to having background on a single example?\n",
        "- How does the overall performance get affected on the entire test set?\n",
        "- How could performance be improved on the test set?\n",
        "\n",
        "Bonus exercises :\n",
        "- Test the model on a test set in which example contain more polygons than the data used to train the model. Does the model generalize well to a higher number of polygons? How high can you increase the number of polygons before the performance of the model is severely affected?\n",
        "- Finetune the model on data with non-white background instead. How does performance vary on the test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq87kYZCPbBX"
      },
      "source": [
        "## Instance Segmentation\n",
        "\n",
        "We will now use the binary masks to do instance segmentation instead of object detection. In instance segmentation, the goal is to not only detect an object, but also to determine which pixels correspond to that very object. Since we are defining our polygons using binary masks, we will be able to use those as our ground truth training data.\n",
        "\n",
        "The architecture we will use is Mask-RCNN. It is similar to Faster-RCNN but has an additional branch for segmentation. The architecture sketch is shown here:\n",
        "\n",
        "![maskrcnn](https://github.com/jerpint/ivado-mila-dl-school-2019-vancouver/blob/dlschool21/assests/maskrcnn.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNK7IpAsVPpT"
      },
      "source": [
        "### Model declaration\n",
        "\n",
        "We will begin by defining our model. Just like with Faster RCNN, we will use a pre-trained model and replace the appropriate parts of the network. We will have to replace both the object detector portion of the network as well as the mask prediction portion of the network to be adapted to our task.\n",
        "\n",
        "In this case, there are 2 parts of the network we will be replacing - `FasterRCNNPredictor`, just like in the previous example, which will do bounding box regression, as well as the `MaskRCNNPredictor` which will be used for segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upFD7_xgU8Dd"
      },
      "source": [
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "def get_model_instance_segmentation(num_classes, pretrained=True, box_score_thresh=0.7):\n",
        "\n",
        "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
        "        pretrained=pretrained, box_score_thresh=box_score_thresh\n",
        "      )\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained box detector with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-_8lO79WNE2"
      },
      "source": [
        "# Define our datasets with the different splits\n",
        "dataset_len = 100\n",
        "batch_size = 2\n",
        "\n",
        "\n",
        "# Instantiate training dataset and training dataloader\n",
        "dataset_train = Polygons(im_width, im_height, dataset_len=dataset_len, split='train', background=True)\n",
        "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=utils.collate_fn)\n",
        "\n",
        "# Instantiate validation dataset and validation dataloader\n",
        "dataset_val = ... # To complete.\n",
        "data_loader_val = ... # To complete.\n",
        "\n",
        "\n",
        "# Instantiate test dataset and test dataloader\n",
        "dataset_test = ... # To complete.\n",
        "data_loader_test = ... # To complete.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQy4dmNBEHrK"
      },
      "source": [
        "Let's look at some of the predicted masks of the network before training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqedqqHLnFAo"
      },
      "source": [
        "# get the model using our helper function\n",
        "num_classes = dataset_train.max_polygon_side - 1\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = get_model_instance_segmentation(num_classes, box_score_thresh=0.5)\n",
        "model.to(device);\n",
        "\n",
        "# Look at predictions on the test set before fine-tuning\n",
        "dl_iter = iter(data_loader_test)\n",
        "\n",
        "for i in range(3):\n",
        "    images, _ = next(dl_iter)\n",
        "    \n",
        "    model.eval()\n",
        "    images = [image.to(device) for image in images]\n",
        "    predictions = model(images)           # Returns predictions\n",
        "    display(display_image_and_boxes(images[0], predictions[0]))\n",
        "    display(display_masks(images[0], predictions[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaJ2a-a1BLsK"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tarosoTpBTuB"
      },
      "source": [
        "#### Exercise 9\n",
        "\n",
        "Construct an optimizer and a learning rate scheduler. \n",
        "Use SGD with `lr=0.005`, `momentum=0.9` and `weight_decay=0.0005` for the optimizer.\n",
        "Use `step_size=3` and `gamma=0.1` for the learning rate scheduler (hint: use StepLR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMknt1aym5Sg"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = ... # To complete.\n",
        "lr_scheduler = ... # To complete.\n",
        "\n",
        "# let's train it for 5 epochs\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnDm9yOOErNX"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Now that the training is done, lets look at the quality of our predictions - we will plot bounding boxes overlayed on top of the original images as well as segmentation masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wyoQm7Hlchv"
      },
      "source": [
        "dl_iter = iter(data_loader_test)\n",
        "\n",
        "for n in range(3):\n",
        "    images, _ = next(dl_iter)\n",
        "  \n",
        "    model.eval()\n",
        "    images = [image.to(device) for image in images]\n",
        "    predictions = model(images)           # Returns predictions\n",
        "    display(display_masks(images[0], predictions[0]))\n",
        "    display(display_image_and_boxes(images[0], predictions[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsLwCjnbWkk4"
      },
      "source": [
        "# References\n",
        "Various parts of this tutorial are inspired from these other tutorials: \n",
        "* https://github.com/andrewliao11/dni.pytorch/blob/master/mlp.py\n",
        "* https://github.com/andrewliao11/dni.pytorch/blob/master/cnn.py\n",
        "* http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "* http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "* http://cs231n.github.io/convolutional-networks/\n",
        "* http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#convolution-as-a-matrix-operation\n"
      ]
    }
  ]
}